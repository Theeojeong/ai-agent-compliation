# ğŸ§  ì‹¤ì‚¬ìš© ì¤‘ì‹¬ AI Agent ì»¬ë ‰ì…˜

ì‹¤ì œ ì—…ë¬´/í•™ìŠµ í™˜ê²½ì—ì„œ ë°”ë¡œ ì“¸ ìˆ˜ ìˆëŠ” ì—ì´ì „íŠ¸ë“¤ì„ ë§Œë“¤ì–´ ë³´ì•˜ìŠµë‹ˆë‹¤.

í¬í•¨ëœ ì—ì´ì „íŠ¸: `content-pipline-agent`, `job-hunter-agent`, `news-reader-agent`

---

## ğŸ“¦ 1. content-pipline-agent

- í”„ë¡œì íŠ¸ëª…: "content-pipline-agent"
- ì„¤ëª…: Firecrawl ê¸°ë°˜ì˜ ì‹ ë¢°ë„ ë†’ì€ ì›¹ ê²€ìƒ‰/ìŠ¤í¬ë© íˆ´ì„ ê°–ì¶˜ ì½˜í…ì¸  ë¦¬ì„œì¹˜ íŒŒì´í”„ë¼ì¸. ë§ˆí¬ë‹¤ìš´ì„ ì •ì œí•´ ìš”ì•½/ë¶„ë¥˜ ë“± í›„ì† íƒœìŠ¤í¬ì— ì í•©í•œ ì…ë ¥ìœ¼ë¡œ ì œê³µí•©ë‹ˆë‹¤.

### ì„¤ì¹˜ ë° ì‹¤í–‰

```bash
cd content-pipline-agent
uv sync
touch .env
uv run python main.py
```

í•„ìš” ë³€ìˆ˜: `OPENAI_API_KEY`, `FIRECRAWL_API_KEY`

### ì¤‘ìš”í•œ ì½”ë“œ

Firecrawl ê²€ìƒ‰ ê²°ê³¼ë¥¼ ì •ì œí•˜ëŠ” ë„êµ¬:

```python
@tool
def web_search_tool(query: str):
    app = FirecrawlApp(api_key=os.getenv("FIRECRAWL_API_KEY"))
    response = app.search(query=query, limit=5, scrape_options=ScrapeOptions(formats=["markdown"]))
    if not response.success:
        return "Error using tool."

    cleaned_chunks = []
    for result in response.data:
        cleaned = re.sub(r"\\+|\n+", "", result["markdown"]).strip()
        cleaned = re.sub(r"\[[^\]]+\]\([^\)]+\)|https?://[^\s]+", "", cleaned)
        cleaned_chunks.append({
            "title": result["title"],
            "url": result["url"],
            "markdown": cleaned,
        })
    return cleaned_chunks
```

ìš”ì 

- ê²€ìƒ‰+ìŠ¤í¬ë© ê²°ê³¼ì—ì„œ ê°œí–‰/ë§í¬/URLì„ ì œê±°í•´ ëª¨ë¸ ì¹œí™” í…ìŠ¤íŠ¸ë¡œ ì •ì œí•©ë‹ˆë‹¤.
- CrewAI íƒœìŠ¤í¬ì—ì„œ ê³§ë°”ë¡œ í˜¸ì¶œ ê°€ëŠ¥í•œ `@tool` í˜•íƒœë¡œ ì œê³µë©ë‹ˆë‹¤.

---

## ğŸ’¼ 2. job-hunter-agent

- í”„ë¡œì íŠ¸ëª…: "job-hunter-agent"
- ì„¤ëª…: ê³µê³  ìˆ˜ì§‘ â†’ ë§¤ì¹­ ì ìˆ˜í™” â†’ í¬ì§€ì…˜ ì„ íƒ â†’ ì´ë ¥ì„œ ë¦¬ë¼ì´íŠ¸ â†’ ê¸°ì—… ë¶„ì„ â†’ ë©´ì ‘ ì¤€ë¹„ê¹Œì§€ ì´ì–´ì§€ëŠ” í’€ íŒŒì´í”„ë¼ì¸ ë©€í‹° ì—ì´ì „íŠ¸. `pydantic` ìŠ¤í‚¤ë§ˆë¡œ ì¶œë ¥ êµ¬ì¡°ë¥¼ ì—„ê²©íˆ ë³´ì¥í•©ë‹ˆë‹¤.

### ì„¤ì¹˜ ë° ì‹¤í–‰

```bash
cd job-hunter-agent
uv sync
touch .env
uv run python main_reference.py
```

í•„ìš” ë³€ìˆ˜: `OPENAI_API_KEY`, `FIRECRAWL_API_KEY`

### íŒŒì´í”„ë¼ì¸ ê°œìš”

1. job_search_agent: Firecrawl ë„êµ¬ë¡œ ê³µê³  ìˆ˜ì§‘
2. job_matching_agent: ì •í•©ì„± ì ìˆ˜(1~5)ì™€ ì‚¬ìœ  ë¶€ì—¬
3. job_selection_task: ìµœì  ê³µê³  ì„ íƒ
4. resume_optimization_agent: ê³µê³  ë§ì¶¤ ì´ë ¥ì„œ ë¦¬ë¼ì´íŠ¸
5. company_research_agent: íšŒì‚¬ ì‹ í˜¸/í•µì‹¬ ì£¼ì œ ë¶„ì„
6. interview_prep_agent: ì§ˆë¬¸/ì–´í•„ í¬ì¸íŠ¸ ë¸Œë¦¬í•‘

### ì¤‘ìš”í•œ ì½”ë“œ

ê°•íƒ€ì… ì¶œë ¥ ìŠ¤í‚¤ë§ˆ:

```python
class Job(BaseModel):
    job_title: str
    company_name: str
    job_location: str
    job_posting_url: str
    job_summary: str

class JobList(BaseModel):
    jobs: List[Job]

class RankedJob(BaseModel):
    job: Job
    match_score: int
    reason: str

class RankedJobList(BaseModel):
    ranked_jobs: List[RankedJob]

class ChosenJob(BaseModel):
    job: Job
    selected: bool
    reason: str
```

ìš”ì 

- ê° ë‹¨ê³„ ì¶œë ¥ì´ ìŠ¤í‚¤ë§ˆë¡œ ê²€ì¦ë˜ì–´ ë‹¤ìš´ìŠ¤íŠ¸ë¦¼ ì˜¤ë¥˜ë¥¼ ì¤„ì…ë‹ˆë‹¤.
- ì›ë³¸ í•„ë“œë¥¼ ë³´ì¡´í•˜ê³  `match_score`, `reason`ë§Œ ì¶”ê°€í•´ íˆ¬ëª…ì„±ì„ í™•ë³´í•©ë‹ˆë‹¤.

---

## ğŸ“° 3. news-reader-agent

- í”„ë¡œì íŠ¸ëª…: "news-reader-agent"
- ì„¤ëª…: ì£¼ì œ ê¸°ë°˜ ë‰´ìŠ¤ ìˆ˜ì§‘ â†’ 3ë‹¨ê³„ ìš”ì•½(Headline/Executive/Comprehensive) â†’ ì—ë””í† ë¦¬ì–¼ íë ˆì´ì…˜ ìë™í™”. Serper ê²€ìƒ‰ + Playwright ìŠ¤í¬ë©ìœ¼ë¡œ í­ë„“ê³  ì‹ ë¢°ë„ ë†’ì€ ìˆ˜ì§‘ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.

### ì„¤ì¹˜ ë° ì‹¤í–‰

```bash
cd news-reader-agent
uv sync
uv run playwright install
touch .env
uv run python main.py
```

í•„ìš” ë³€ìˆ˜: `OPENAI_API_KEY`, `SERPER_API_KEY`

### ì¤‘ìš”í•œ ì½”ë“œ

ê°„ê²°í•œ ìŠ¤í¬ë ˆì´í¼:

```python
search_tool = SerperDevTool(n_results=30)

@tool
def scrape_tool(url: str):
    with sync_playwright() as p:
        browser = p.chromium.launch(headless=True)
        page = browser.new_page()
        page.goto(url)
        time.sleep(5)
        html = page.content()
        browser.close()
        soup = BeautifulSoup(html, 'html.parser')
        for tag in soup.find_all(["header","footer","nav","aside","script","style","noscript","iframe",
                                   "form","button","input","select","textarea","img","svg","canvas","audio",
                                   "video","embed","object"]):
            tag.decompose()
        content = soup.get_text(separator=" ")
        return content or "No content"
```

ìš”ì 

- ë³¸ë¬¸ í…ìŠ¤íŠ¸ë§Œ ë‚¨ê¸°ë„ë¡ ë¶ˆí•„ìš”í•œ DOM ìš”ì†Œë¥¼ ì œê±°í•©ë‹ˆë‹¤.
- í›„ì† íƒœìŠ¤í¬ì—ì„œ ë‹¤ë‹¨ê³„ ìš”ì•½ê³¼ ìµœì¢… ë¦¬í¬íŠ¸ í¸ì§‘ì„ ì§„í–‰í•©ë‹ˆë‹¤.

---

## ğŸ’¬ 4. chatgpt-clone

- í”„ë¡œì íŠ¸ëª…: "chatgpt-clone"
- ì„¤ëª…: Streamlit ê¸°ë°˜ì˜ ì±„íŒ… UIë¡œ, `openai-agents`ë¥¼ í™œìš©í•´ ì›¹ ê²€ìƒ‰(WebSearchTool)ê³¼ íŒŒì¼ ê²€ìƒ‰(FileSearchTool)ì„ ê²°í•©í•©ë‹ˆë‹¤. ëŒ€í™” ì´ë ¥ì€ ë¡œì»¬ SQLiteì— ì €ì¥ë˜ë©°, ìŠ¤íŠ¸ë¦¬ë° ì‘ë‹µê³¼ ì§„í–‰ ìƒíƒœ í‘œì‹œë¥¼ ì œê³µí•©ë‹ˆë‹¤.

### ì‹¤í–‰

URL: https://theeojeong-ai-agent-compliation-chatgpt-clonemain-zxfczg.streamlit.app/

### ì¤‘ìš”í•œ ì½”ë“œ

Agent/ì„¸ì…˜ ì´ˆê¸°í™” ìš”ì•½:

```python
agent = Agent(
    name="ChatGPT Clone",
    instructions="You are a helpful assistant...",
    tools=[
        WebSearchTool(),
        FileSearchTool(vector_store_ids=[VECTOR_STORE_ID], max_num_results=3)
    ]
)

session = SQLiteSession("user1", "user-memory.db")
```

ìš”ì 

- Streamlit ì±„íŒ… UI + íŒŒì¼ ì—…ë¡œë“œ(.txt) ì§€ì›
- ì›¹/íŒŒì¼ ê²€ìƒ‰ í˜¸ì¶œ ì‹œ ìƒíƒœ ë©”ì‹œì§€ë¡œ ì§„í–‰ ìƒí™© ê°€ì‹œí™”
- ë¡œì»¬ SQLite(`user-memory.db`)ì— ëŒ€í™” ë©”ëª¨ë¦¬ ì˜êµ¬ ì €ì¥

---

## ğŸ™Œ ê¸°ì—¬ ë°©ë²•

```bash
git checkout -b feat/<feature-name>
git commit -m "feat: <ì„¤ëª…>"
git push origin feat/<feature-name>
```

ì´ìŠˆ/PR í™˜ì˜í•©ë‹ˆë‹¤!
